---
title: "DATA607 Sentiment Analysis"
author: "Christian Uriostegui"
date: "2022-11-06"
output: html_document
---

# Assignment

Use the primary example code from Chapter 2 of "Text Mining with R" from Julia Silge & David Robinson and extended the code in two ways:

-   Work with a different corpus of your choosing

-   Incorporate at least one additional sentiment lexicon

# Text Mining with R

Example code taken from [**https://www.tidytextmining.com/sentiment.html**](https://www.tidytextmining.com/sentiment.html){.uri} **"Text Mining with R: A Tidy Approach" by Julia Silge and David Robinson**

# Load Libraries

```{r library}
library(tidyverse)
library(tidytext)
library(janeaustenr)
library(dplyr)
library(stringr)
library(tidyr)
library(ggplot2)
library(hcandersenr)
library(SentimentAnalysis)
library(textdata)
```

# Example Code from Chapter 2

### Loading Jane Austen collection

```{r}
tidy_books <- austen_books() %>%
  group_by(book) %>%
  mutate(linenumber = row_number(),
  chapter = cumsum(str_detect(text,                 regex("^chapter[\\divxlc]",ignore_case = TRUE)))) %>%
  ungroup() %>%
  unnest_tokens(word, text)
```

### Examining Joy Words in the book Emma

```{r}
nrc_joy <- get_sentiments("nrc") %>% 
  filter(sentiment == "joy")

tidy_books %>%
  filter(book == "Emma") %>%
  inner_join(nrc_joy) %>%
  count(word, sort = TRUE)
```

### Using "bing" Lexicon for Sentiment Analysis

```{r}
jane_austen_sentiment <- tidy_books %>%
  inner_join(get_sentiments("bing")) %>%
  count(book, index = linenumber %/% 80, sentiment) %>%
  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %>% 
  mutate(sentiment = positive - negative)
```

### Comparing Sentiment Count in "nrc" & "bing" Lexicon

```{r}
get_sentiments("nrc") %>% 
  filter(sentiment %in% c("positive", "negative")) %>% 
  count(sentiment)
```

### Most Common Positive and Negative Words

```{r}
bing_word_counts <- tidy_books %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  ungroup()

bing_word_counts
```

```{r}
bing_word_counts %>%
  group_by(sentiment) %>%
  slice_max(n, n = 10) %>% 
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(x = "Contribution to sentiment",
       y = NULL)
```

------------------------------------------------------------------------

# My Analysis

I chose to explore a package that contains all 157 of H.C. Andersen's fairy tale works. The dataset contains his work in 5 languages: Danish, German, English, Spanish and French. To begin my analysis, I filtered the dataset for the English version of the texts.

# hcandersenr Dataset
Credit to Emil Hvitfedlt for the hcandersenr package:
[**https://github.com/emilhvitfeldt/hcandersenr**](https://github.com/emilhvitfeldt/hcandersenr) hcandersenr by Emil Hvitfeldt


### Downloading Dataset

```{r}
# filtered for the english version of the text
hcane <- hca_fairytales() %>%
  group_by(book) %>%
  filter(language == "English") %>%
  mutate(linenumber = row_number()) %>%
  ungroup()
```

### Tokenization

Now that we have our text in the language of interest, we begin the process of tokenization.

```{r}
tidy_hcane <- hcane %>%
  unnest_tokens(word, text)
```

### Removal of Stop Words

```{r}
data(stop_words)
```

```{r}
tidy_hcane <- tidy_hcane %>%
  anti_join(stop_words)
```

```{r}
tidy_hcane %>%
  group_by(book) %>%
  count(word, sort = TRUE)

tidy_hcane
```

### Downloading Lexicon for Assignment

For my analysis, I decided to use the DictionaryGI lexicon from the Sentiment Analysis package. Upon inspection, I can see that DictionaryGI dictionary is in a list. There are 2005 negative words and 1637 positive words - which can pose a problem if I wish to join them together in a dataframe.

```{r}
data(DictionaryGI)
str(DictionaryGI)
```

### Cleaning Lexicon Dataframe

To avoid errors when turning the list into a dataframe, I made the lengths of both rows the same.

```{r}
length(DictionaryGI$positive) <- length(DictionaryGI$negative)
```

I then turned the list object into a dataframe.

```{r}
DictionaryGI_df <- as.data.frame(DictionaryGI)
```

I realized I needed to transform the presentation of dataframe for my analysis later. In order to do this, I would have to put the variables "positive" and "negative" into their own columns. My approach was to seperate the words by sentiment, add a column that had the sentiment that applied to that group, rename the columns and then join them. Once joined, I then removed the NA values.

```{r}
negative <- DictionaryGI_df$negative
negative <- as.data.frame(negative)
negative <- negative %>% 
  mutate(sentiment = "negative") %>%
  rename("word"="negative")
```

```{r}
positive <- DictionaryGI_df$positive
positive <- as.data.frame(positive)
positive <- positive %>% 
  mutate(sentiment="positive") %>%
  rename("word"="positive")
```

```{r join}
Lex_DictionaryGI <- bind_rows(positive, negative)

Lex_DictionaryGI <- Lex_DictionaryGI %>%
  na.omit()
```

### Final Dataset for Analysis

```{r}
books_sentiment <- tidy_hcane %>%
  inner_join(Lex_DictionaryGI) %>%
  group_by(book) %>%
  count(sentiment)
```

### Top 5 Books with Most Negative & Positive Words

Before creating the visuals, I seperate H.C. Andersen's text by sentiment values and then arranged them in descending order, limiting the results to only 5.

```{r}
books_pos <- books_sentiment %>%
  filter(sentiment=="positive") %>%
  arrange(desc(n)) %>%
  head(5)
```

```{r}
books_neg <- books_sentiment %>%
  filter(sentiment == "negative") %>%
  arrange(desc(n)) %>%
  head(5)
```

```{r}
top_five <- bind_rows(books_pos, books_neg)
```

An interesting observation is that, with the exception of the lower two variables, the top 3 books that contain negative works also appears in the same order for containing the most positive words.

```{r}
ggplot(top_five,mapping = aes(x = reorder(book, desc(n)), y = n)) + 
geom_col() + 
facet_grid(~sentiment,scales = "free") +
theme(axis.text.x = element_text(size = 6))
```

The amount of total negative words in the author's collection exceeds the total positive words found.

```{r}
Lex_DictionaryGI %>%
  count(sentiment)
```

Looking at the most frequent words that appeared in the text as well as it's paired sentiment value, I was suprised by some of the labeling such as "stood" being assigned as positive, and "hand" as negative. I'm interested about the context in which the words appear. That likely played an impact in their sentiment assignment.

```{r}
dictGI_count <- tidy_hcane %>%
  inner_join(Lex_DictionaryGI) %>%
  count(word, sentiment, sort = TRUE) %>%
  ungroup()

dictGI_count
```

We can see in the visuals, that despite negative words appearing more frequently in the text, a positive word ("stood") appear the most, when comparing both sentiments.

```{r}
dictGI_count %>%
  group_by(sentiment) %>%
  slice_max(n, n = 10) %>% 
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(x = "Sentiment Analysis",
       y = NULL)
```
